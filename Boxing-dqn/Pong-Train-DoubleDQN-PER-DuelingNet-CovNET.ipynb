{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DoubleDQN with Prioritized Replay (PER) and Dueling Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random, time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).to(device)\n",
    "\n",
    "from common.wrappers import make_atari, wrap_deepmind, wrap_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Prioritized Memory Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePrioritizedBuffer(object):\n",
    "    def __init__(self, capacity, prob_alpha=0.6):\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity   = capacity\n",
    "        self.buffer     = []        self.pos        = 0\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        assert state.ndim == next_state.ndim\n",
    "        state      = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        \n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "        \n",
    "        self.priorities[self.pos] = max_prio\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "        \n",
    "        probs  = prios ** self.prob_alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total    = len(self.buffer)\n",
    "        weights  = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights  = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch       = list(zip(*samples))\n",
    "        states      = np.concatenate(batch[0])\n",
    "        actions     = batch[1]\n",
    "        rewards     = batch[2]\n",
    "        next_states = np.concatenate(batch[3])\n",
    "        dones       = batch[4]\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, indices, weights\n",
    "    \n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Lambda Functions for Epsilon Greedy Strategy and Annealing of PER's beta parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8f70f57828>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "beta_start = 0.4\n",
    "beta_frames = 1000 \n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "\n",
    "plt.plot([beta_by_frame(i) for i in range(10000)])\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "plt.plot([epsilon_by_frame(i) for i in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classes for DQN and Dueling DQN (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_actions):\n",
    "        super(CnnDQN, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),frame_idx\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action\n",
    "    \n",
    "class DuelingCnnDQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_outputs):\n",
    "        super(DuelingCnnDQN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.num_actions = num_outputs\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "        return value + advantage  - advantage.mean()\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0))\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].data[0]\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Functions for computing the temporal difference loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def compute_td_loss(batch_size, beta):\n",
    "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta) \n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "    weights    = Variable(torch.FloatTensor(weights))\n",
    "\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = target_model(next_state)\n",
    "\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss  = F.smooth_l1_loss(q_value, expected_q_value.detach(), reduction='none') * weights\n",
    "    prios = loss + 1e-5\n",
    "    loss  = loss.mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def compute_td_loss_double_huber(batch_size, beta):\n",
    "    \"\"\"\n",
    "    For double DQN we use the policy network for estimating the  current and next q values\n",
    "    whilst using the target network also to estimate the next q values.\n",
    "    The estimates for the next q values from the policy network are then used to pick the next action whilst the\n",
    "    estimate from the policy nextwork is then used to rate how good that action was.\n",
    "    \"\"\"\n",
    "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta)\n",
    "\n",
    "    state      = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\n",
    "    action     = Variable(torch.LongTensor(action))\n",
    "    reward     = Variable(torch.FloatTensor(reward))\n",
    "    done       = Variable(torch.FloatTensor(done))\n",
    "    weights    = Variable(torch.FloatTensor(weights))\n",
    "\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    next_q_state_values = target_model(next_state) \n",
    "\n",
    "    q_value       = q_values.gather(1, action.unsqueeze(1)).squeeze(1) \n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    loss  = F.smooth_l1_loss(q_value, expected_q_value.detach(), reduction='none') * weights\n",
    "    prios = loss + 1e-5\n",
    "    loss = loss.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions for plotting and model updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    ax1 = plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    ax2 = plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    ax1.set_ylim(0,100)\n",
    "    ax2.set_ylim(0,100)\n",
    "    plt.show()\n",
    "    \n",
    "def update_target(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DQN and save model after various training periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Hyperparameters\n",
    "num_frames = 1000000\n",
    "buffer_size = 400000\n",
    "batch_size = 32\n",
    "gamma      = 0.99\n",
    "learning_rate = 0.001 \n",
    "update_targets = 1000\n",
    "replay_initial = 10000\n",
    "\n",
    "#Plotting Controls\n",
    "plot_frequency = 2500\n",
    "render = False\n",
    "\n",
    "#initialize counters/trackers etc\n",
    "losses = []\n",
    "all_rewards = []\n",
    "duration = []\n",
    "episode_reward = 0\n",
    "timer = time.time()\n",
    "\n",
    "#Build Env\n",
    "env_id = \"PongNoFrameskip-v0\"\n",
    "env    = make_atari(env_id)\n",
    "env    = wrap_deepmind(env)\n",
    "env    = wrap_pytorch(env)\n",
    "\n",
    "#Build Networks\n",
    "current_model = DuelingCnnDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model  = DuelingCnnDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model.eval()    \n",
    "\n",
    "#Align Weights in target network to match the policy network\n",
    "update_target(current_model, target_model)\n",
    "\n",
    "#Build Optimizer\n",
    "optimizer = optim.Adam(current_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#Build Replay Buffer (Prioritized)\n",
    "replay_buffer  = NaivePrioritizedBuffer(buffer_size)\n",
    "\n",
    "#Start Training\n",
    "state = env.reset()\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    \n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = current_model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        duration.append(time.time() - timer)\n",
    "        timer = time.time()\n",
    "        \n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        beta = beta_by_frame(frame_idx)\n",
    "        loss = compute_td_loss_double_huber(batch_size, beta)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    if frame_idx % plot_frequency == 0:\n",
    "        plot(frame_idx, all_rewards, losses)\n",
    "        print(\"Average Episode Duration: %s\" % np.mean(duration[-10:]))\n",
    "        \n",
    "    if frame_idx % update_targets == 0:\n",
    "        update_target(current_model, target_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

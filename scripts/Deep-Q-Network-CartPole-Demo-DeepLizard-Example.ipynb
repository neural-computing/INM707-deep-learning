{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test building a RL algorithym for frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PyTorch to build the Policy network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24) # in_features takes the state (image size * channels)\n",
    "        self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
    "        self.out = nn.Linear(in_features=32, out_features=2)     # out_features on final layer is the number of actions\n",
    "\n",
    "    def forward(self, t):\n",
    "        t = t.flatten(start_dim=1)\n",
    "        t = F.relu(self.fc1(t))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Experience Class (for use within Replay Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "                        'Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward')\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Replay Memory Class to store the Experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity                                    # we set a capacity for the memory\n",
    "        self.memory = []                                            # this holds the stored experiences\n",
    "        self.position = 0                                           # lets keep track of how many we've added\n",
    "\n",
    "    def push(self, experience):                                                 # function to 'push' experience into memory\n",
    "        \"\"\"Saves a transition.\"\"\" \n",
    "        if len(self.memory) < self.capacity:                                    # if there is capacity to store the experience\n",
    "            self.memory.append(experience)                                            #     - add to memory\n",
    "        else:\n",
    "            self.memory[self.position % self.capacity] = experience             # otherwise, push new experiences onto front of memory\n",
    "            self.position += 1\n",
    "\n",
    "    def sample(self, batch_size):                                  # return a random batch\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):                      # return a boolean of if we are able to return a batch\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Class for epsilon greedy: exploit vs explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "        \n",
    "    def get_exploration_rate(self, current_step):\n",
    "        return self.end + (self.start - self.end) * math.exp(-1. * current_step * self.decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions, device):\n",
    "        \"\"\"\n",
    "            strategy - EpsilonGreedyStrategy\n",
    "            num_actions - number of possible actions\n",
    "        \"\"\"\n",
    "        self.current_step = 0\n",
    "        self.strategy = strategy\n",
    "        self.num_actions = num_actions\n",
    "        self.device = device\n",
    "        \n",
    "    def select_action(self, state, policy_network):\n",
    "        \"\"\"\n",
    "        Select the next action that the agent should take: explorative or exploitive.\n",
    "        \n",
    "        If we choose to exploit, then we return the highest q-value from the policy network for the given state.\n",
    "        \n",
    "        The torch.no_grad() step is requires since we are using the policy network for inference and not training\n",
    "        at this point.\n",
    "        \"\"\"\n",
    "        rate = strategy.get_exploration_rate(self.current_step)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if rate > random.random():\n",
    "            action = random.randrange(self.num_actions)  # explore\n",
    "            return torch.tensor([action]).to(device)     # return a tensor\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_network(state).argmax(dim=1).to(device) # exploit leveraging the correct device (pytorch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the required functions for the state preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnvManager():\n",
    "    def __init__(self, device):\n",
    "        \"\"\"\n",
    "        Initialize the class variables\n",
    "        \n",
    "        This encapsulation allows the manager to have the required functionality\n",
    "        over the environment and avoids us having to handle some functionality here\n",
    "        and other directly with the gym env\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.env = gym.make(\"CartPole-v0\").unwrapped\n",
    "        self.env.reset()\n",
    "        self.current_screen = None     # this will track the current screen of the env. None = start of episode\n",
    "        self.done = False              # indicates if any action taken has ended the episode\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Wrapper Function for the gym reset functionality\"\"\"\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Wrapper Function for the gym close functionality\"\"\"\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"Wrapper Function for the gym render functionality\"\"\"\n",
    "        return self.env.render(mode)\n",
    "    \n",
    "    def num_actions_available(self):\n",
    "        \"\"\"Return the number of actions that are available to the agent at a given time\"\"\"\n",
    "        return self.env.action_space.n\n",
    "    \n",
    "    def take_action(self, action):\n",
    "        \"\"\"\n",
    "        Execute the selected action within the environment\n",
    "        \n",
    "        Note that action will be a tensor so action.item() will return the tensor value from pytorch\n",
    "        \n",
    "        We just return the reward as a new tensor. Ensures that the datatype is consistent within main program\n",
    "        \"\"\"\n",
    "        _, reward, self.done, _ = self.env.step(action.item())\n",
    "        return torch.tensor([reward], device=self.device)\n",
    "    \n",
    "    def just_starting(self):\n",
    "        \"\"\"Helper function to tell us if we are at the beginning of the episode\"\"\"\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Return the current state of the environment in the form of a processed\n",
    "        image of the screen.\n",
    "        \n",
    "        A state consists of the difference between two frames.\n",
    "        \"\"\"\n",
    "        if self.just_starting() or self.done:\n",
    "            #if were at beginning or end of an episode, show a black screen\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            #otherwise we want to return the difference between the frames as the state\n",
    "            #and update the current_screen to be equal to the incoming frame.\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "        \n",
    "    def get_screen_height(self):\n",
    "        \"\"\"Wrapper function to get processed screen height\"\"\"\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        \"\"\"Wrapper function to get processed screen height\"\"\"\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "    \n",
    "    def get_processed_screen(self):\n",
    "        \"\"\"\n",
    "        Render a rgb_array from gym and then transpose into the order of \n",
    "        channels, heigh, width before cropping and transforming the cropped image\n",
    "        \"\"\"\n",
    "        screen = self.render(\"rgb_array\").transpose((2, 0, 1))\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        \"\"\"Wrapper to crop screen top and bottom if not needed\"\"\"\n",
    "        screen_height = screen.shape[1]\n",
    "        \n",
    "        #strip off top and bottom \n",
    "        top = int(screen_height * 0.4)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        screen = screen[:, top:bottom, :]\n",
    "        return screen\n",
    "    \n",
    "    def transform_screen_data(self, screen):\n",
    "        \"\"\"\n",
    "        Function to transform the screen data\n",
    "        \n",
    "        Note:\n",
    "        we call np.ascontiguousarray so that the screen is converted to numpy array of same\n",
    "        size and stored sequentially in memory. We convert to float32 and then scale the pixel data\n",
    "        we then convert it to a tensor using the numpy bridge\n",
    "        \"\"\"\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        #we use torchvision to compose a set of image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()          # convert to PIL image\n",
    "            , T.Resize((40, 90))    # resize the image to 40x90\n",
    "            , T.ToTensor()          # convert to tensor\n",
    "        ])\n",
    "        \n",
    "        # we return the resized screen and then call unsqueeze to add another dimention to the tensor\n",
    "        # this extra dimention represents a batch dimention since the preprocessed images will be \n",
    "        # passed to the DQN in batches. Since this also a tensor, we call to(device) like normal\n",
    "        return resize(screen).unsqueeze(0).to(self.device)\n",
    "    \n",
    "    def get_image_from_screen(self, screen):\n",
    "        \"\"\"Wrapper function to squeeze and return image from screen/state\"\"\"\n",
    "        return screen.squeeze(0).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example of a preprocessed screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)\n",
    "em.reset()\n",
    "screen = em.render(\"rgb_array\")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(screen)\n",
    "plt.title(\"None-Processed screen example\")\n",
    "plt.show()\n",
    "\n",
    "screen = em.get_processed_screen()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(em.get_image_from_screen(screen), interpolation=\"none\")\n",
    "plt.title(\"Processed screen example\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show an example of start, intermediate and end \"STATES\" - this is what will be fed into the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the start state\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(em.get_image_from_screen(screen), interpolation=\"none\")\n",
    "plt.title(\"Start state example\")\n",
    "plt.show()\n",
    "\n",
    "#show the end state\n",
    "em.done = True\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(em.get_image_from_screen(screen), interpolation=\"none\")\n",
    "plt.title(\"End state example\")\n",
    "plt.show()\n",
    "\n",
    "#show a intermediate state \n",
    "em.done = False\n",
    "for i in range(5):\n",
    "    em.take_action(torch.tensor([1]))\n",
    "\n",
    "screen = em.get_state()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(em.get_image_from_screen(screen), interpolation=\"none\")\n",
    "plt.title(\"State after taking action '1' 5 time\\n(difference between two frames)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some utility functions for performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(values, moving_avg_period):\n",
    "    \"\"\"\n",
    "    Wrapper function to plot the reward\n",
    "    Here reward = +1 for every step that doesnt end the episode\n",
    "    so the reward is directly correlated to the length of a episode\n",
    "    \"\"\"\n",
    "    moving_avg = get_moving_average(moving_avg_period, values)\n",
    "    plt.figure(2)\n",
    "    plt.clf\n",
    "    plt.title(\"Training...\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Duration\")\n",
    "    plt.plot(values)\n",
    "    plt.plot(moving_avg)\n",
    "    plt.pause(0.001)\n",
    "    print(\"Episode\", len(values), \"\\n\", moving_avg_period, \"episode moving avg:\", moving_avg[-1])\n",
    "    if is_ipython: display.clear_output(wait=True)\n",
    "\n",
    "def get_moving_average(period, values):\n",
    "    \"\"\"\n",
    "    Utility function to compute the moving average of a set of values\n",
    "    over a period.\n",
    "    \n",
    "    If the period is smaller than the number of values, we unfold the tensor\n",
    "    across the 0th dimention, creating a set of slices of size 'period'\n",
    "    we can then use these to calculate the mean of each slice then flatten back\n",
    "    we concatenate period-1 zeros onto the front of the array so that we know that\n",
    "    the moving average corresponds the datapoints after this initial period.\n",
    "\n",
    "    If our period is larger than our value array, we cannot calculate a moving average so return zeros\n",
    "    \"\"\"\n",
    "    values = torch.tensor(values, dtype=torch.float)\n",
    "    if len(values) >= period:\n",
    "        moving_avg = values.unfold(dimension=0, size=period, step=1).mean(dim=1).flatten(start_dim=0)\n",
    "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
    "        return moving_avg.numpy()\n",
    "    else:\n",
    "        moving_avg = torch.zeros(len(values))\n",
    "        return moving_avg.numpy()\n",
    "\n",
    "def extract_tensors(experiences):\n",
    "    \"\"\"\n",
    "    Function to transpose a 'batch of experiences' into\n",
    "    and 'experience of batches'\n",
    "    \n",
    "    i.e\n",
    "    FROM\n",
    "    [Experience(state=1, action=1, next_state=1, reward=1)\n",
    "    ,Experience(state=2, action=2, next_state=2, reward=2)]\n",
    "    \n",
    "    TO\n",
    "    Experience(state=(1, 2), action=(1, 2), next_state=(1, 2), reward=(1, 2))\n",
    "    \n",
    "    Using torch.cat, we then convert each NamedTuple into a tensor and return as a tuple\n",
    "    \"\"\"\n",
    "    batch = Experience(*zip(*experiences))\n",
    "    \n",
    "    t1 = torch.cat(batch.state)\n",
    "    t2 = torch.cat(batch.action)\n",
    "    t3 = torch.cat(batch.reward)\n",
    "    t4 = torch.cat(batch.next_state)\n",
    "    \n",
    "    return (t1, t2, t3, t4)\n",
    "\n",
    "class QValues():\n",
    "    \"\"\"\n",
    "    This class is used\n",
    "        - To get the current q-values by feeding the current states into the policy network\n",
    "        - To get the next q-values by feeding the next states into the target network\n",
    "    \n",
    "    We use staticmethod magic since we are not going to create an instance of this class\n",
    "    and just want to use it as a container for these methods.\n",
    "    \n",
    "    As a result we initialize device in the class to save having it as an argument\n",
    "    \n",
    "    Remember: next_states is a tensor of all possible \"next_state\" and we want to pick the best q-value from each state within the batch\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current(policy_network, states, actions):                                                          \n",
    "        return policy_network(states).gather(dim=1, index=actions.unsqueeze(-1))                               # return the predicted q-values\n",
    "\n",
    "    @staticmethod\n",
    "    def get_next(target_network, next_states):\n",
    "        # find the locations of the final states - we dont want to pass these through the target network\n",
    "        # this is because we know that the predicted q-value for these final states will be zero\n",
    "        final_state_locations = next_states.flatten(start_dim=1).max(dim=1)[0].eq(0).type(torch.bool)          # flatten next_states and search for where the max == 0. This is a final state so mark with True, rest false\n",
    "        non_final_state_locations = (final_state_locations == False)                                           # opposite tensor to final_state_locations. We how have True in position of non_final_states\n",
    "        non_final_states = next_states[non_final_state_locations]                                              # now that we know the locations of the non-final states, we can extract them\n",
    "        batch_size = next_states.shape[0]                                                                      # find the batch size\n",
    "        values = torch.zeros(batch_size).to(QValues.device)                                                    # create a values of zeros of the same size\n",
    "        values[non_final_state_locations] = target_network(non_final_states).max(dim=1)[0].detach()            # for each state in batch, update the elements that correspond to non-final states\n",
    "        return values                                                                                          # to the maximum predicted q-values for that state. 0 if final state, highest q-value for state (across all actions) if non-final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo the plot utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data and moving average\n",
    "#shows that the moving average is 0 for the first 100 episodes\n",
    "plot(np.random.rand(300),100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.002\n",
    "target_update = 10\n",
    "memory_size = 100000\n",
    "lr = 0.001\n",
    "num_episodes = 1000\n",
    "\n",
    "\n",
    "# Create Objects\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")                # set device for pytorch\n",
    "em = CartPoleEnvManager(DEVICE)                                                      # Build CartPole EnvManager\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)                      # Set Strategy\n",
    "agent = Agent(strategy, em.num_actions_available(), DEVICE)                          # Create Agent\n",
    "memory = ReplayMemory(memory_size)                                                   # Create Replaymemory\n",
    "\n",
    "#Create Pytorch Networks\n",
    "policy_network = DQN(em.get_screen_height(), em.get_screen_width()).to(DEVICE)         # Build Policy Network\n",
    "target_network = DQN(em.get_screen_height(), em.get_screen_width()).to(DEVICE)         # Build Target Network\n",
    "target_network.load_state_dict(policy_network.state_dict())                          # Set parameters in Target Network to those in Policy Network\n",
    "target_network.eval()                                                                # Put Target Network into Eval Model so Pytorch knows we are only using this for inference\n",
    "optimizer = optim.Adam(params=policy_network.parameters(), lr=lr)                    # Build the Network Optimizer to update weights after backprop\n",
    "\n",
    "#Start Main Loop of Training\n",
    "episode_durations = []                                                               # List to collect the episode durations in - proxy for reward and used in tracking performance over tie\n",
    "\n",
    "for episode in range(num_episodes):                                                  # Step 1 - Loop through Episodes\n",
    "    em.reset()                                                                       # Step 2 - Reset Environment at beginning of episode\n",
    "    state = em.get_state()                                                           # Step 3 - Get the starting state of the environment\n",
    "    \n",
    "    for timestep in count():                                                         # Step 4 - For each timestep in the episode\n",
    "        action = agent.select_action(state, policy_network)                          #        - Select the agent's action\n",
    "        reward = em.take_action(action)                                              #        - Take this action in the environment and return the reward\n",
    "        next_state = em.get_state()                                                  #        - Get the new state of the environment after taking this action\n",
    "        memory.push(Experience(state, action, next_state, reward))                   #        - Capture this \"experience\" in the Replay Memory\n",
    "        state = next_state                                                           #        - Update the state variable ready for the next timestep\n",
    "        \n",
    "        if memory.can_provide_sample(batch_size):                                    #         Step 5 - IF the replay memory has enough samples to build a batch\n",
    "            experiences = memory.sample(batch_size)                                  #                - Sample replay memory to build a training batch\n",
    "            states, actions, rewards, next_states = extract_tensors(experiences)     #                - Preprocess the batch from a batch of Experiences to an Experience of Batches - see doc string on function\n",
    "            \n",
    "            current_q_values = QValues.get_current(policy_network, states, actions)  #                - get q values from the current state, action pairs within the batch\n",
    "            next_q_values =  QValues.get_next(target_network, next_states)           #                - get the next q values from the next state\n",
    "            target_q_values = (next_q_values * gamma ) + rewards                     #                - build optimal q value term from bellman equation\n",
    "            \n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))        #                - calculate the loss as MSE\n",
    "            optimizer.zero_grad()                                                    #                - set gradients of all weights and bias' to zero - needs to be called before backward. \n",
    "            loss.backward()                                                          #                - compute the gradient of the loss w.r.t. all parameters in the policy network\n",
    "            optimizer.step()                                                         #                - update the optimizer with the gradients calculated during backprop\n",
    "            \n",
    "        if em.done:\n",
    "            episode_durations.append(timestep)                                       #         Step 6 - capture duration once episode is complete\n",
    "            plot(episode_durations, 100)                                             #                - plot the result\n",
    "            break                                                                    #                - break since out end of episode\n",
    "            \n",
    "    if episode % target_update == 0:\n",
    "        target_network.load_state_dict(policy_network.state_dict())                  #      Step 7 - update the target network weights if every 'target_update'th episode\n",
    "        \n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test building a RL algorithym for frozen lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import retro\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring the env object\n",
    "def print_env_doc_strings(env):\n",
    "    #We can see the rewards available from the end\n",
    "    print(\"---------------ENV INFO---------------\\n\")\n",
    "    print(env.unwrapped.__doc__)    \n",
    "    \n",
    "    #We can see the rewards available from the end\n",
    "    print(\"--------------------------------------\\n\\n-------------REWARD RANGE-------------\\n\")\n",
    "    print(env.reward_range.__doc__)\n",
    "\n",
    "    #we can see the action_space available\n",
    "    print(\"--------------------------------------\\n\\n-------------ACTION SPACE-------------\\n\")\n",
    "    print(env.action_space.__doc__)\n",
    "\n",
    "    #we can see the observation/state space\n",
    "    print(\"-------------------------------------\\n\\n-------------STATE SPACE-------------\\n\")\n",
    "    print(env.observation_space.__doc__)\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------ENV INFO---------------\n",
      "\n",
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n",
      "--------------------------------------\n",
      "\n",
      "-------------REWARD RANGE-------------\n",
      "\n",
      "Built-in immutable sequence.\n",
      "\n",
      "If no argument is given, the constructor returns an empty tuple.\n",
      "If iterable is specified the tuple is initialized from iterable's items.\n",
      "\n",
      "If the argument is a tuple, the return value is the same object.\n",
      "--------------------------------------\n",
      "\n",
      "-------------ACTION SPACE-------------\n",
      "\n",
      "A discrete space in :math:`\\{ 0, 1, \\\\dots, n-1 \\}`. \n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> Discrete(2)\n",
      "\n",
      "    \n",
      "-------------------------------------\n",
      "\n",
      "-------------STATE SPACE-------------\n",
      "\n",
      "A discrete space in :math:`\\{ 0, 1, \\\\dots, n-1 \\}`. \n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> Discrete(2)\n",
      "\n",
      "    \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#this gives us the possible states, actions and rewards\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "print_env_doc_strings(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 20000\n",
    "max_steps_per_episode = 100\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "\n",
    "chunks = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 0.14725890356142457\n",
      "5000 0.5592\n",
      "7500 0.6708\n",
      "10000 0.6736\n",
      "12500 0.696\n",
      "15000 0.678\n",
      "17500 0.678\n",
      "20000 0.7036\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "#Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done=False\n",
    "    rewards_current_episode = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "\n",
    "        #env.render()\n",
    "        # Exploration vs Exploitation step\n",
    "        if random.uniform(0,1) > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-table for Q(s,a)\n",
    "        q_table[state,action] = (q_table[state, action] * (1 - learning_rate) + \n",
    "                                 learning_rate * \n",
    "                                 (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "                                )\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Exploration Rate decacy ready for next episode\n",
    "    exploration_rate = (min_exploration_rate + \n",
    "                        (max_exploration_rate - min_exploration_rate) * \n",
    "                        np.exp(-exploration_decay_rate * episode)\n",
    "                       )\n",
    "    if episode % chunks == (chunks-1) or episode == num_episodes-1:\n",
    "        rewards_per_100_episodes = np.array(rewards_all_episodes[-chunks:])\n",
    "        print(episode+1, np.mean(rewards_per_100_episodes))\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61550054 0.53079561 0.52825869 0.53474988]\n",
      " [0.41397556 0.36225655 0.32808077 0.55202637]\n",
      " [0.41584028 0.44405689 0.42441994 0.50706228]\n",
      " [0.24429734 0.25650916 0.32289427 0.47608504]\n",
      " [0.62178767 0.42908285 0.3765692  0.34631378]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41112109 0.19016877 0.15888744 0.129621  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.52761134 0.48082925 0.34549211 0.63693605]\n",
      " [0.44467049 0.68446795 0.54957577 0.49434036]\n",
      " [0.62549205 0.52244802 0.45312668 0.39391986]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.53485107 0.57662585 0.75811757 0.64378772]\n",
      " [0.70829595 0.86990351 0.77191149 0.77815989]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#we can then print the learned optimal policy - matrix of state vs action\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "****You reached the goal!****\n"
     ]
    }
   ],
   "source": [
    "#Now lets watch the trained agent play the game\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
